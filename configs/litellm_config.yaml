model_list:
  # llama.cpp local models (always available)
  # Phi-4 Q6_K (supports function calling) - PRIMARY MODEL
  - model_name: phi-4-Q6_K
    litellm_params:
      model: openai/phi-4-Q6_K.gguf
      api_base: http://llamacpp:8080/v1
      api_key: "not-required"
      temperature: 0.1
      top_p: 0.95
      max_tokens: 2048

  # Phi-4 Q4_K_M (supports function calling) - ALTERNATIVE
  - model_name: phi-4-Q4_K_M
    litellm_params:
      model: openai/phi-4-Q4_K_M.gguf
      api_base: http://llamacpp:8080/v1
      api_key: "not-required"
      max_tokens: 2048
  
  # SmolLM3-3B (supports function calling, multilingual, reasoning)
  - model_name: SmolLM3-3B-128K-BF16
    litellm_params:
      model: openai/SmolLM3-3B-128K-BF16.gguf
      api_base: http://llamacpp:8080/v1
      api_key: "not-required"
      # SmolLM3 recommended settings
      temperature: 0.6
      top_p: 0.95
      max_tokens: 2048

  # DeepSeek-R1 (reasoning model)  
  - model_name: DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL
    litellm_params:
      model: openai/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf
      api_base: http://llamacpp:8080/v1
      api_key: "not-required"
      # DeepSeek recommended settings
      temperature: 0.6
      top_p: 0.95
      max_tokens: 2048

  # Qwen3-14B (enhanced reasoning and tool calling)
  - model_name: Qwen3-14B-Q5_K_M
    litellm_params:
      model: openai/Qwen3-14B-Q5_K_M.gguf
      api_base: http://llamacpp:8080/v1
      api_key: "not-required"
      # Qwen3 recommended settings
      temperature: 0.6
      top_p: 0.95
      max_tokens: 2048

  # Hugging Face Models (VERIFIED WORKING with token)
  - model_name: bart-summarizer
    litellm_params:
      model: huggingface/facebook/bart-large-cnn
      api_key: os.environ/HF_TOKEN
      temperature: 0.6
      max_tokens: 150

  # WORKING: HF models as OpenAI-compatible endpoints
  - model_name: smollm3-3b
    litellm_params:
      model: openai/HuggingFaceTB/SmolLM3-3B
      api_base: https://api-inference.huggingface.co/models/HuggingFaceTB/SmolLM3-3B/v1
      api_key: os.environ/HF_TOKEN
      temperature: 0.6
      max_tokens: 2048

  # Testing official LiteLLM HF format (from docs)
  - model_name: smollm3-official
    litellm_params:
      model: huggingface/HuggingFaceTB/SmolLM3-3B
      api_key: os.environ/HF_TOKEN
      temperature: 0.6
      max_tokens: 2048

  # Qwen3-32B via Cerebras (PRIMARY LOCAL-FIRST CHAT MODEL)
  - model_name: qwen3-32b
    litellm_params:
      model: huggingface/cerebras/Qwen/Qwen3-32B
      api_key: os.environ/HF_TOKEN
      temperature: 0.6
      max_tokens: 2048

  # HuggingFace Embedding Models (LOCAL-FIRST MEMORY SYSTEM)
  # Qwen3-Embedding-4B - #1 MTEB multilingual leaderboard (PRIMARY)
  - model_name: qwen3-embedding-4b
    litellm_params:
      model: huggingface/Qwen/Qwen3-Embedding-4B
      api_key: os.environ/HF_TOKEN
      
  # Qwen3-Embedding-8B - Best performance (ADVANCED)
  - model_name: qwen3-embedding-8b
    litellm_params:
      model: huggingface/Qwen/Qwen3-Embedding-8B
      api_key: os.environ/HF_TOKEN
      
  # Qwen3-Embedding-0.6B - Fastest/smallest (RESOURCE-CONSTRAINED)
  - model_name: qwen3-embedding-0.6b
    litellm_params:
      model: huggingface/Qwen/Qwen3-Embedding-0.6B
      api_key: os.environ/HF_TOKEN

  # Google Gemini models are now added dynamically via API when valid Google API key is available
  # This prevents exposure of unusable models when no Google API key is configured

general_settings:
  master_key: "sk-1234"
  
  # Enable UI for monitoring
  ui_access_mode: "admin_only"
  
  # Default model fallback strategy (updated dynamically when Gemini models are available)
  fallbacks:
    # No fallbacks defined here - managed dynamically by LLM service

litellm_settings:
  # Enable detailed logging
  set_verbose: true
  
  # Request timeout settings
  request_timeout: 60
  
  # Retry settings
  num_retries: 3